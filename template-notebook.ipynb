{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742adba1-54cd-47b6-b445-d82cb2250c64",
   "metadata": {},
   "source": [
    "# Template Notebook for Milestones\n",
    "\n",
    "In this notebook you will write your code, producing the required output for each Milestone.\n",
    "\n",
    "Your notebook must contain 3 types of cells:\n",
    "\n",
    "- (1) Code cells: Cells that contain code snippets, capturing one cohesive fragment of your code.\n",
    "\n",
    "- (2) Corresponding explanation cells: Each code cell must be followed by a text cell containing the **English** explanation of what the corresponding code cell does and what it's purpose is\n",
    "\n",
    "- (3) One reflection cell: One cell at the bottom of the notebook that contains your individual reflection on your process working on this milestones in **English**. It could contain technical problems and how you overcame them, it could contain social problems and how you deal with them (group work is hard!), it could contain explanations of prior skills or knowledge that made certain parts of the task easier for you, etc... (those are just suggestions. Your individual reflections will of course contain different/additional aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dc1eff2",
   "metadata": {},
   "source": [
    "# Import\n",
    "The above code cell is needed to import necessary libraries to pre-process the data. Since we are dealing with a **'.jsonl'** file, we need to use the **'json.load()'** function to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90444d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "\n",
    "with open('/home/mark/Projects/Python/information_retrieval/luckycoincidence/ir-anthology-07-11-2021-ss23.jsonl') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        raw_dataset.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9228f2dc",
   "metadata": {},
   "source": [
    "# Loading and Parsing JSONL Dataset\n",
    "\n",
    "This code is used to read in a JSONL dataset from a specified file path, where each line in the file contains a JSON object. \n",
    "\n",
    "The **'open()'** function opens the file, iterates through each line using a for loop, and parses each line using **'json.loads()'**. The resulting dictionary object is added to the **'raw_dataset'** array.\n",
    "\n",
    "The **'raw_dataset'** array is used to store the raw data, which can then be further processed and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f464578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset[0])\n",
    "print(raw_dataset[1])\n",
    "print(raw_dataset[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20af251b",
   "metadata": {},
   "source": [
    "# Retrieving Initial Dataset\n",
    "\n",
    "The cell above is to show the first 3 lines of data and the varibles contained in them. Since the data is already parsed into an array, we can now easily access each elements by their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90444d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "\n",
    "for doc in raw_dataset:\n",
    "    doc_id = doc['id']\n",
    "    title = doc['title']\n",
    "    abstract = doc['abstract']\n",
    "    text = title + '. ' + abstract\n",
    "    new_doc = {'doc_id': doc_id, 'text': text}\n",
    "    new_dataset.append(new_doc)\n",
    "\n",
    "with open('/home/mark/Projects/Python/information_retrieval/luckycoincidence/processed_dataset.jsonl', 'w') as f:\n",
    "    for doc in new_dataset:\n",
    "        json.dump(doc, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d5a244d",
   "metadata": {},
   "source": [
    "# Pre-Processing Data\n",
    "The cell above which extracts selected variables from 'raw_dataset'. **'id'** is extracted and renamed as **'doc_id'**, while **'title'** and **'abstract'** are combined and then renamed as **'text'**. This creates our new dataset. Then, we write this dataset into a jsonl file named **'processed_dataset.jsonl'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e399262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:32:49.147957942Z",
     "start_time": "2023-04-29T14:32:49.052407271Z"
    }
   },
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from ir_datasets.formats import JsonlDocs, TrecXmlQueries\n",
    "from ir_datasets.util import PackageDataFile\n",
    "from ir_datasets.datasets.base import Dataset\n",
    "\n",
    "ir_datasets.registry.register(\n",
    "    'iranthology-luckycoincidence',\n",
    "    Dataset(\n",
    "        JsonlDocs(PackageDataFile('luckycoincidence/processed_dataset.jsonl')),\n",
    "        TrecXmlQueries(PackageDataFile('luckycoincidence/topics.xml')),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "019e478b",
   "metadata": {},
   "source": [
    "# Registering Lucky Coincidence Dataset in **'ir_datasets'**\n",
    "This code registers the processed dataset and TREC XML-format topics for the **Lucky Coincidence** team into **'ir_datasets'**. \n",
    "\n",
    "The processed dataset and topics are loaded using the **'JsonlDocs'** and **'TrecXmlQueries'** modules respectively, and are packaged using **'PackageDataFile'**. \n",
    "\n",
    "Then, the dataset is registered with the name **'iranthology-luckycoincidence'** into **'ir_datasets'** using the register function from the **'ir_datasets.registry'** module. \n",
    "\n",
    "This makes the dataset available for use in future experiments and tasks, allowing researchers to easily access and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output processed dataset path\n",
    "print(f'Processed dataset saved to \"/home/mark/Projects/Python/information_retrieval/luckycoincidence/processed_dataset.jsonl\"')\n",
    "\n",
    "# Output TREC XML-format topics\n",
    "with open('/home/mark/Projects/Python/information_retrieval/luckycoincidence/topics.xml', 'r') as f:\n",
    "    xml_string = f.read()\n",
    "    \n",
    "with open('/home/mark/Projects/Python/information_retrieval/luckycoincidence/custom_topics.xml', 'w') as f:\n",
    "    f.write(xml_string)\n",
    "\n",
    "print('TREC XML-format topics saved to \"/home/mark/Projects/Python/information_retrieval/luckycoincidence/custom_topics.xml\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f31a5e50",
   "metadata": {},
   "source": [
    "# Outputting Processed Dataset and TREC XML-format Topics\n",
    "\n",
    "The **first line outputs** the path where the processed dataset is saved in JSONL format. \n",
    "\n",
    "The **second block reads** the 'TREC XML-format topics file saved in the file path and converts it to a string. \n",
    "\n",
    "The **third block writes** the TREC XML-format topics string to a new file path where it can be saved. \n",
    "\n",
    "Finally, **the last line outputs** the path where the TREC XML-format topics are saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1f614e3-7fe6-4d84-9f25-47fc9b341ccb",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Working on this project was difficult and confusing in some parts. While it was relatively simple to go through the steps of creating a new .jsonl dataset and topics in .xml format, we were challenged by the assignment in parts where we had to use Docker and TIRA. We have never created a Dockerfile before and ran into a lot of problems trying to utilize the ir_datasets library and building a Docker image. And due to our time limit of less than one week and very limited amount of contact with the experts, we had to rely a lot on doing independent trial and error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
