{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742adba1-54cd-47b6-b445-d82cb2250c64",
   "metadata": {},
   "source": [
    "# Template Notebook for Milestones\n",
    "\n",
    "In this notebook you will write your code, producing the required output for each Milestone.\n",
    "\n",
    "Your notebook must contain 3 types of cells:\n",
    "\n",
    "- (1) Code cells: Cells that contain code snippets, capturing one cohesive fragment of your code.\n",
    "\n",
    "- (2) Corresponding explanation cells: Each code cell must be followed by a text cell containing the **English** explanation of what the corresponding code cell does and what it's purpose is\n",
    "\n",
    "- (3) One reflection cell: One cell at the bottom of the notebook that contains your individual reflection on your process working on this milestones in **English**. It could contain technical problems and how you overcame them, it could contain social problems and how you deal with them (group work is hard!), it could contain explanations of prior skills or knowledge that made certain parts of the task easier for you, etc... (those are just suggestions. Your individual reflections will of course contain different/additional aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fa832d-c0ed-41dd-b4a9-68580c1ae974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc83362",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "\n",
    "with open('/home/mark/Projects/Python/Information Retrieval/ir-anthology/ir-anthology-07-11-2021-ss23.jsonl') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        raw_dataset.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdab554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crossref': 'DBLP:conf/sigir/2019birndl', 'booktitle': 'Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019', 'series': 'CEUR Workshop Proceedings', 'volume': '2414', 'pages': '196â€“207', 'publisher': 'CEUR-WS.org', 'year': '2019', 'url': 'http://ceur-ws.org/Vol-2414/paper20.pdf', 'biburl': 'https://dblp.org/rec/conf/sigir/LiZXHLLL19.bib', 'bibsource': 'dblp computer science bibliography, https://dblp.org', 'bibkey': 'DBLP:conf/sigir/LiZXHLLL19', 'bibtype': 'inproceedings', 'pdf': 'http://ceur-ws.org/Vol-2414/paper20.pdf', 'authors': ['Lei Li', 'Yingqi Zhu', 'Yang Xie', 'Zuying Huang', 'Wei Liu', 'Xingyuan Li', 'Yinan Liu'], 'editors': [], 'venue': 'SIGIR', 'id': '2019.sigirconf_workshop-2019birndl.21', 'date': 1581522299.0, 'abstract': 'Starting from its former version, CIST@CLSciSumm-18, our CIST@CLSciSumm-19 system is going to participate in the shared Task 1A (citation linkage), Task 1B (facet classification) and Task 2 (summarization) in CLSciSumm-19@SIGIR2019. We mainly try to improve its methods for all the shared tasks. We build a new feature of Word2vec H for the CNN model to calculate sentence similarity for citation linkage. We plan to adopt CNN and RNN variants for facet classification. And in order to improve the performance of summarization, we develop more semantic representations for sentences based on neural network language models to construct new kernel matrix used in Determinantal Point Processes (DPPs).', 'title': 'CIST@CLSciSumm-19: Automatic Scientific Paper Summarization with Citances and Facets'}\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90444d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "\n",
    "for doc in raw_dataset:\n",
    "    doc_id = doc['id']\n",
    "    title = doc['title']\n",
    "    abstract = doc['abstract']\n",
    "    text = title + '. ' + abstract\n",
    "    new_doc = {'doc_id': doc_id, 'text': text}\n",
    "    new_dataset.append(new_doc)\n",
    "\n",
    "with open('/home/mark/Projects/Python/Information Retrieval/Lucky Coincidence/processed_dataset.jsonl', 'w') as f:\n",
    "    for doc in new_dataset:\n",
    "        json.dump(doc, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e399262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:32:49.147957942Z",
     "start_time": "2023-04-29T14:32:49.052407271Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ir_datasets' has no attribute 'IndexedText'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trec_topics_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 28\u001b[0m ir_datasets\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mregister(\u001b[43mMyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mMyDataset.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mark/Projects/Python/Information Retrieval/Lucky Coincidence/processed_dataset.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trec_topics_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mark/Projects/Python/Information Retrieval/Lucky Coincidence/topics.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miranthology-luckycoincidence\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     {\n\u001b[0;32m---> 10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mir_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexedText\u001b[49m(ir_datasets\u001b[38;5;241m.\u001b[39mbase_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docs_path), {\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m         }),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrec_topics\u001b[39m\u001b[38;5;124m'\u001b[39m: ir_datasets\u001b[38;5;241m.\u001b[39mTrecXmlQueries(ir_datasets\u001b[38;5;241m.\u001b[39mbase_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trec_topics_path)),\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ir_datasets' has no attribute 'IndexedText'"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "class MyDataset(ir_datasets.Dataset):\n",
    "    def __init__(self):\n",
    "        self._docs_path = '/home/mark/Projects/Python/Information Retrieval/Lucky Coincidence/processed_dataset.jsonl'\n",
    "        self._trec_topics_path = '/home/mark/Projects/Python/Information Retrieval/Lucky Coincidence/topics.xml'\n",
    "        super().__init__('iranthology-luckycoincidence',\n",
    "            {\n",
    "                'docs': ir_datasets.TsvDocs({\n",
    "                    'doc_id': 0,\n",
    "                    'text': 1,\n",
    "                }),\n",
    "                'trec_topics': ir_datasets.TrecXmlQueries(),\n",
    "        },\n",
    "        {\n",
    "            'docs': self._docs_path,\n",
    "            'trec_topics': self._trec_topics_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def docs_iter(self):\n",
    "        with open(self._docs_path) as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                yield {'doc_id': doc['doc_id'], 'text': doc['text']}\n",
    "\n",
    "    def trec_topics(self):\n",
    "        with open(self._trec_topics_path) as f:\n",
    "            return f.read()\n",
    "\n",
    "ir_datasets.registry.register(MyDataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440f20e-3c64-44c3-8537-f5065f792f8a",
   "metadata": {},
   "source": [
    "### Example Explanation cell\n",
    "\n",
    "The above cell prints a sentence in order to give an impression of what might be done with such a cell. This cell here explains it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab4a36-0f0b-46d2-99a4-56e792f70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcc247-ee10-4db4-ac59-876377b5ce9c",
   "metadata": {},
   "source": [
    "### Add More Explanation cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f614e3-7fe6-4d84-9f25-47fc9b341ccb",
   "metadata": {},
   "source": [
    "### Example Reflection Cell\n",
    "\n",
    "Working on this notebook was difficult, because it is not the real notebook but just an example. My experience in writing notebooks helped speed up the process, for example knowing that there are different types of cells. However, it was difficult to figure our what to write exactly in order to make sure the students understand what they are supposed to do as I could not test it before giving it to the students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9c7e9-8228-4f4f-b942-d460a35eee31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
